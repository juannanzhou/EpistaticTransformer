{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b779bb7-5bf6-4be9-a45f-63b7a0f9c296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import random as rd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import GPUtil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('../model')\n",
    "from utils import amino_acid_to_number, tokenize\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../model')\n",
    "from functions import get_A2N_list, tokenize, make_train_val_test_lists_rand, prepare_data\n",
    "from models import ProtDataset\n",
    "\n",
    "outpath = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338bcbd6-8ed4-40cf-98e1-f786813d2502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.makedirs(outpath + study_id + \"_rep_\" + str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9d3493-6847-457d-83a7-a12115d25da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_name = \"Faure2023_1_lenient\"\n",
    "train_percent = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0efc44-351d-470a-941f-94a45c14c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make folder for storing analysis outputs\n",
    "\n",
    "study_id = \"_\".join([data_name, str(train_percent) + \"%\"])\n",
    "\n",
    "matching_folders = [folder for folder in os.listdir(outpath) if study_id in folder and os.path.isdir(os.path.join(outpath, folder)) ]\n",
    "\n",
    "if len(matching_folders) == 0:\n",
    "    rep = 0\n",
    "else: rep = np.max([int(folder.split(\"_\")[-1]) for folder in matching_folders]) + 1\n",
    "\n",
    "results_path = outpath + \"_\".join([study_id, \"rep\", str(rep)])\n",
    "os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152d4940-66ed-469f-87cc-1a65888a33be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R2s = pd.DataFrame(columns=['Model', 'R2'])\n",
    "R2s.to_csv(os.path.join(results_path, 'R2s.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55f18f-5014-475c-b735-7b379069f56a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef29dba2-4aae-4e50-af30-318326a339dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_path = \"../Data/Data_prepared/\" + data_name + \".csv\"\n",
    "datafile = pd.read_csv(in_path, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ffe419-4de8-488c-8e83-914a7c847bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/juannanzhou/ProteinLLE/notebooks/../model/functions.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  seqs = seqs[:, sites_var]\n"
     ]
    }
   ],
   "source": [
    "phenotypes, seqs, seqs1h = prepare_data(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b4da8b-87d1-4945-be92-a568ae517377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length = 34;  AA_size = 2\n"
     ]
    }
   ],
   "source": [
    "_, L, AA_size = seqs1h.shape\n",
    "print(f\"sequence length = {L}; \", f\"AA_size = {AA_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e9fdec-233a-4d8b-b6d4-f05b4c30a1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25864\n"
     ]
    }
   ],
   "source": [
    "num_train = int(.01*train_percent*len(datafile))\n",
    "num_test = 2000\n",
    "train_list, val_list, test_list = make_train_val_test_lists_rand(datafile, num_train, num_test)    \n",
    "print(num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9362a62-6230-457f-95f3-917191d7c62b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d7faa5-f745-403a-bd0a-a366891f007d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Linear\"\n",
    "from models import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff72fb6b-46d0-45ff-868b-ea43ed2c8cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "X = seqs1h.float().to(device)\n",
    "y = phenotypes.to(device)\n",
    "\n",
    "X_train, y_train = X[train_list], y[train_list]\n",
    "X_val, y_val = X[val_list], y[val_list]\n",
    "X_test, y_test = X[test_list], y[test_list]\n",
    "\n",
    "\n",
    "train_dataset = ProtDataset(X_train, y_train)\n",
    "train_loader = data.DataLoader(train_dataset,\n",
    "                               batch_size=1000,\n",
    "                               shuffle=True,\n",
    "                               drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0f069b9-87ba-41bc-813d-daa72b18fe57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropout_p = 0.0\n",
    "model = LinearModel(L, AA_size, dropout_p).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35cc8c37-1c73-4f50-b9f6-c1a9c965cf25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from models import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e56d6152-9e09-4af6-8f99-1323766f1c07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6456961929798126\n",
      "0.006871824042067844\n",
      "Epoch 11/300, Loss: 0.26104320992123\n",
      "0.006732416532868313\n",
      "Epoch 21/300, Loss: 0.2609321136366237\n",
      "0.10676915831027803\n",
      "Epoch 31/300, Loss: 0.2589776692065326\n",
      "0.3646081428152946\n",
      "Epoch 41/300, Loss: 0.11678294295614416\n",
      "0.5721909380724145\n",
      "Epoch 51/300, Loss: 0.11415663361549377\n",
      "0.5748013816128043\n",
      "Epoch 61/300, Loss: 0.11293533444404602\n",
      "0.5784241291034687\n",
      "Epoch 71/300, Loss: 0.11225231398235667\n",
      "0.582069313158436\n",
      "Epoch 81/300, Loss: 0.11187286471778696\n",
      "0.5835685444347483\n",
      "Epoch 91/300, Loss: 0.11160500008951534\n",
      "0.584386002243876\n",
      "Epoch 101/300, Loss: 0.11124361103231256\n",
      "0.5851785991392328\n",
      "Epoch 111/300, Loss: 0.11113004860552875\n",
      "0.5872695072721136\n",
      "Epoch 121/300, Loss: 0.11100862513888966\n",
      "0.585480210974256\n",
      "Epoch 131/300, Loss: 0.11089750582521612\n",
      "0.5866639571734393\n",
      "Epoch 141/300, Loss: 0.111132873730226\n",
      "0.5860542009164328\n",
      "Epoch 151/300, Loss: 0.1109411818060008\n",
      "0.5874863395103854\n",
      "Epoch 161/300, Loss: 0.11129309914328835\n",
      "0.587949751573138\n",
      "Epoch 171/300, Loss: 0.11124001985246484\n",
      "0.5889690950004207\n",
      "Epoch 181/300, Loss: 0.11116073348305443\n",
      "0.5878958300894135\n",
      "Epoch 191/300, Loss: 0.11137810823592273\n",
      "0.5888361058232521\n",
      "Epoch 201/300, Loss: 0.11100348085165024\n",
      "0.5881358794287218\n",
      "Epoch 211/300, Loss: 0.11101705784147436\n",
      "0.5872848284348995\n",
      "Epoch 221/300, Loss: 0.11212161454287442\n",
      "0.5895878667229838\n",
      "Epoch 231/300, Loss: 0.11089519953185861\n",
      "0.5886229878570189\n",
      "Epoch 241/300, Loss: 0.11129701340740378\n",
      "0.5884802928808688\n",
      "Epoch 251/300, Loss: 0.11121294715187767\n",
      "0.589054397050623\n",
      "Epoch 261/300, Loss: 0.1109822304411368\n",
      "0.5890698449232059\n",
      "Epoch 271/300, Loss: 0.111109351569956\n",
      "0.5882842794485647\n",
      "Epoch 281/300, Loss: 0.11084964939139107\n",
      "0.5889036071523293\n",
      "Epoch 291/300, Loss: 0.11086143214594234\n",
      "0.5888477155353703\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "learning_rate = 0.01\n",
    "epochs = 300\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "        model.eval()\n",
    "        pred, true = model(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "        print(pearsonr(pred, true)[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e5f8be4-3f8b-4a7e-9fa4-609e3c4fb9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model achieved test R2 = 0.6111873340301814\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred, true = model(X_test.flatten(1)).flatten().detach().cpu().numpy(), y_test.flatten().detach().cpu().numpy()\n",
    "\n",
    "r2_test = pearsonr(pred, true)[0]**2\n",
    "\n",
    "print(f\"{model_name} model achieved test R2 = {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3165239d-c904-41a8-b3a3-f23045a02c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(os.path.join(results_path, \"R2s.csv\"), mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([[model_name, r2_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2d6b7-b01a-439a-b4ca-5df94e3b7cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d033f1bc-4a5e-466d-914f-e36263467fa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c504b67e-f6de-4f8b-a8b3-158343101f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from models import Transformer_torch_MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c819d3-ceaf-4730-9926-388e774ea692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs_ex = seqs + AA_size*torch.tensor(range(L))\n",
    "X = seqs_ex.to(device)\n",
    "y = phenotypes.to(device)\n",
    "X_train, y_train = X[train_list], y[train_list]\n",
    "X_val, y_val = X[val_list], y[val_list]\n",
    "X_test, y_test = X[test_list], y[test_list]\n",
    "train_dataset = ProtDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca1eda99-7078-457d-83b5-3248c3136622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "batch_size = 544\n",
    "train_loader = data.DataLoader(train_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True,\n",
    "                               drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f987feff-29f8-4fd7-a46f-68220715e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Trial:\n",
    "#   Value: 0.7601\n",
    "#   Params: \n",
    "#     hidden_dim_h: 23\n",
    "#     dropout: 0.12805161023112027\n",
    "#     batch_size: 544\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccaba4c8-d314-405d-9a79-da24b19cf68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sequence_length = L\n",
    "# input_dim = AA_size*L\n",
    "# output_dim = 1\n",
    "# num_layers = 2\n",
    "# num_heads = 4\n",
    "# hidden_dim = 23*num_heads\n",
    "# dropout = 0.12805161023112027\n",
    "\n",
    "# model = Transformer_torch_MHA(L, input_dim, hidden_dim, num_layers, num_heads, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f201473-2c57-4315-8e98-25c596a640bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "# learning_rate = 0.001\n",
    "# epochs = 500\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch_inputs, batch_targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_inputs)\n",
    "#         loss = criterion(outputs, batch_targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     if epoch % 20 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "#         model.eval()\n",
    "#         pred, true = model(X_test.flatten(1)).flatten().detach().cpu().numpy(), y_test.flatten().detach().cpu().numpy()\n",
    "#         print(pearsonr(pred, true)[0]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2885a9-6a23-42b5-82de-e4580432048a",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76019638-99cb-4e89-9a3d-555ade31f6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_heads = 4\n",
    "\n",
    "sequence_length = L\n",
    "input_dim = AA_size*L\n",
    "output_dim = 1\n",
    "\n",
    "def objective(trial):\n",
    "    global criterion_best, model_best\n",
    "\n",
    "    hidden_dim_h = trial.suggest_int('hidden_dim_h', 10, 50)\n",
    "    dropout = trial.suggest_float('dropout', 0.05, 0.35)\n",
    "    batch_size = trial.suggest_int('batch_size', 100, 1200)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 30, 300)\n",
    "    \n",
    "    print(f\"Build model with {num_layers} layers of attention\")\n",
    "    model = Transformer_torch_MHA(L, input_dim, hidden_dim_h*num_heads, num_layers, num_heads, dropout).to(device)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_dataset,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=False)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    r2_test = []\n",
    "    try: \n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for batch_inputs, batch_targets in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_inputs)\n",
    "                    loss = criterion(outputs, batch_targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "                    model.eval()\n",
    "                    pred, true = model(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "                    print(pearsonr(pred, true)[0]**2)\n",
    "                    if pearsonr(pred, true)[0]**2 == \"nan\":\n",
    "                        break\n",
    "                    r2_test.append(pearsonr(pred, true)[0]**2)\n",
    "                    \n",
    "    except: print(\"training failed\")\n",
    "    \n",
    "    criterion = np.array(r2_test)[-1]\n",
    "    if criterion > criterion_best:\n",
    "        print(\"Found better hyperparameter, update model\")\n",
    "        criterion_best = criterion\n",
    "        model_best = model\n",
    "    \n",
    "    return np.array(r2_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e16cd-672c-45fd-a875-b55780d25c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "for num_layers in [1, 2, 3]:\n",
    "\n",
    "    model_name = \"TF_\" + str(num_layers)\n",
    "\n",
    "    criterion_best = 0.\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    best_trial = study.best_trial\n",
    "    print(\"Best Trial:\")\n",
    "    print(f\"  Criterion: {best_trial.value:.4f}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")  \n",
    "\n",
    "    best_hyper_parameters = {}\n",
    "    for key, value in best_trial.params.items():\n",
    "        best_hyper_parameters[key] = value\n",
    "\n",
    "    model_best.eval()\n",
    "    pred, true = model_best(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "\n",
    "    r2_test = pearsonr(pred, true)[0]**2\n",
    "    print(f\"{model_name} achieved R2 = {r2_test}\")\n",
    "\n",
    "    # save test R2 score\n",
    "    import csv\n",
    "    with open(os.path.join(results_path, \"R2s.csv\"), mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([[model_name, r2_test]])\n",
    "\n",
    "    # save predictions\n",
    "    pd.DataFrame({\"prediction\": pred, \"true\": true}).to_csv(os.path.join(results_path, model_name + \"_predictions.csv\"), index=False)\n",
    "\n",
    "    # save best model\n",
    "    torch.save(model_best, os.path.join(results_path, model_name + \"_BestModel\"))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3138b5e-3f12-4aca-9b53-a7497b15c223",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_layers = 1\n",
    "# model_name = \"TF_\" + str(num_layers)\n",
    "\n",
    "# criterion_best = 0.\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# best_trial = study.best_trial\n",
    "# print(\"Best Trial:\")\n",
    "# print(f\"  Criterion: {best_trial.value:.4f}\")\n",
    "# print(\"  Params: \")\n",
    "# for key, value in best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# best_hyper_parameters = {}\n",
    "# for key, value in best_trial.params.items():\n",
    "#     best_hyper_parameters[key] = value\n",
    "\n",
    "# model_best.eval()\n",
    "# pred, true = model_best(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "\n",
    "# r2_test = pearsonr(pred, true)[0]**2\n",
    "# print(f\"{model_name} achieved R2 = {r2_test}\")\n",
    "\n",
    "# # save test R2 score\n",
    "# import csv\n",
    "# with open(os.path.join(results_path, \"R2s.csv\"), mode='a', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerows([[model_name, r2_test]])\n",
    "\n",
    "# # save predictions\n",
    "# pd.DataFrame({\"prediction\": pred, \"true\": true}).to_csv(os.path.join(results_path, model_name + \"_predictions.csv\"), index=False)\n",
    "\n",
    "# # save best model\n",
    "# torch.save(model_best, os.path.join(results_path, model_name + \"_BestModel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c443b-7a04-492d-a4b9-733e2d549a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
