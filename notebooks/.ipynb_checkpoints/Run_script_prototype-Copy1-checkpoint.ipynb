{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b779bb7-5bf6-4be9-a45f-63b7a0f9c296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import random as rd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import GPUtil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('../model')\n",
    "from utils import amino_acid_to_number, tokenize\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../model')\n",
    "from functions import get_A2N_list, tokenize, make_train_val_test_lists_rand, prepare_data\n",
    "from models import ProtDataset\n",
    "\n",
    "outpath = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338bcbd6-8ed4-40cf-98e1-f786813d2502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.makedirs(outpath + study_id + \"_rep_\" + str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9d3493-6847-457d-83a7-a12115d25da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_name = \"Faure2023_1_lenient\"\n",
    "train_percent = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0efc44-351d-470a-941f-94a45c14c023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make folder for storing analysis outputs\n",
    "\n",
    "study_id = \"_\".join([data_name, str(train_percent) + \"%\"])\n",
    "\n",
    "matching_folders = [folder for folder in os.listdir(outpath) if study_id in folder and os.path.isdir(os.path.join(outpath, folder)) ]\n",
    "\n",
    "if len(matching_folders) == 0:\n",
    "    rep = 0\n",
    "else: rep = np.max([int(folder.split(\"_\")[-1]) for folder in matching_folders]) + 1\n",
    "\n",
    "results_path = outpath + \"_\".join([study_id, \"rep\", str(rep)])\n",
    "os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152d4940-66ed-469f-87cc-1a65888a33be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R2s = pd.DataFrame(columns=['Model', 'R2'])\n",
    "R2s.to_csv(os.path.join(results_path, 'R2s.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55f18f-5014-475c-b735-7b379069f56a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef29dba2-4aae-4e50-af30-318326a339dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_path = \"../Data/Data_prepared/\" + data_name + \".csv\"\n",
    "datafile = pd.read_csv(in_path, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ffe419-4de8-488c-8e83-914a7c847bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/juannanzhou/ProteinLLE/notebooks/../model/functions.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  seqs = seqs[:, sites_var]\n"
     ]
    }
   ],
   "source": [
    "phenotypes, seqs, seqs1h = prepare_data(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b4da8b-87d1-4945-be92-a568ae517377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length = 34;  AA_size = 2\n"
     ]
    }
   ],
   "source": [
    "_, L, AA_size = seqs1h.shape\n",
    "print(f\"sequence length = {L}; \", f\"AA_size = {AA_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e9fdec-233a-4d8b-b6d4-f05b4c30a1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25864\n"
     ]
    }
   ],
   "source": [
    "num_train = int(.01*train_percent*len(datafile))\n",
    "num_test = 2000\n",
    "train_list, val_list, test_list = make_train_val_test_lists_rand(datafile, num_train, num_test)    \n",
    "print(num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9362a62-6230-457f-95f3-917191d7c62b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d7faa5-f745-403a-bd0a-a366891f007d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Linear\"\n",
    "from models import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff72fb6b-46d0-45ff-868b-ea43ed2c8cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "X = seqs1h.float().to(device)\n",
    "y = phenotypes.to(device)\n",
    "\n",
    "X_train, y_train = X[train_list], y[train_list]\n",
    "X_val, y_val = X[val_list], y[val_list]\n",
    "X_test, y_test = X[test_list], y[test_list]\n",
    "\n",
    "\n",
    "train_dataset = ProtDataset(X_train, y_train)\n",
    "train_loader = data.DataLoader(train_dataset,\n",
    "                               batch_size=1000,\n",
    "                               shuffle=True,\n",
    "                               drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f069b9-87ba-41bc-813d-daa72b18fe57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropout_p = 0.0\n",
    "model = LinearModel(L, AA_size, dropout_p).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35cc8c37-1c73-4f50-b9f6-c1a9c965cf25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from models import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56d6152-9e09-4af6-8f99-1323766f1c07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.3176604074736436\n",
      "0.044063697893027075\n",
      "Epoch 11/300, Loss: 0.11247230103860299\n",
      "0.5934986206496009\n",
      "Epoch 21/300, Loss: 0.11277942980329196\n",
      "0.5940807199787502\n",
      "Epoch 31/300, Loss: 0.11255488265305758\n",
      "0.5927812725082092\n",
      "Epoch 41/300, Loss: 0.11259598284959793\n",
      "0.5933768256672256\n",
      "Epoch 51/300, Loss: 0.11254267332454522\n",
      "0.5941886584619791\n",
      "Epoch 61/300, Loss: 0.11258724145591259\n",
      "0.5943655054866587\n",
      "Epoch 71/300, Loss: 0.11254456390937169\n",
      "0.5940373214827782\n",
      "Epoch 81/300, Loss: 0.11283398605883121\n",
      "0.5934839689044148\n",
      "Epoch 91/300, Loss: 0.11289972066879272\n",
      "0.5920981122338687\n",
      "Epoch 101/300, Loss: 0.11292321607470512\n",
      "0.5940252018171155\n",
      "Epoch 111/300, Loss: 0.11293516028672457\n",
      "0.5917459719216663\n",
      "Epoch 121/300, Loss: 0.11308929696679115\n",
      "0.591352384183135\n",
      "Epoch 131/300, Loss: 0.11260955625524123\n",
      "0.5934266532800223\n",
      "Epoch 141/300, Loss: 0.11282722186297178\n",
      "0.5929229969971894\n",
      "Epoch 151/300, Loss: 0.11261003309239943\n",
      "0.5940281912245824\n",
      "Epoch 161/300, Loss: 0.11280642574032147\n",
      "0.5935960120247569\n",
      "Epoch 171/300, Loss: 0.11257188580930233\n",
      "0.5945544570276687\n",
      "Epoch 181/300, Loss: 0.11270452383905649\n",
      "0.5934605879990965\n",
      "Epoch 191/300, Loss: 0.11267121508717537\n",
      "0.5944811528673078\n",
      "Epoch 201/300, Loss: 0.11268450630207856\n",
      "0.5921428820011363\n",
      "Epoch 211/300, Loss: 0.11316173678884904\n",
      "0.592599297184997\n",
      "Epoch 221/300, Loss: 0.11269155920793612\n",
      "0.5935195266172607\n",
      "Epoch 231/300, Loss: 0.1125622137139241\n",
      "0.5939497635822387\n",
      "Epoch 241/300, Loss: 0.11266062688082457\n",
      "0.593819746784314\n",
      "Epoch 251/300, Loss: 0.11275943896422784\n",
      "0.5937613025495272\n",
      "Epoch 261/300, Loss: 0.11276139412075281\n",
      "0.5945044734251809\n",
      "Epoch 271/300, Loss: 0.1126316391552488\n",
      "0.5928741062989697\n",
      "Epoch 281/300, Loss: 0.11268709631015857\n",
      "0.5925631600926264\n",
      "Epoch 291/300, Loss: 0.11260903875033061\n",
      "0.5925319988154318\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "learning_rate = 0.01\n",
    "epochs = 300\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "        model.eval()\n",
    "        pred, true = model(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "        print(pearsonr(pred, true)[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5f8be4-3f8b-4a7e-9fa4-609e3c4fb9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model achieved test R2 = 0.5449392844408051\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred, true = model(X_test.flatten(1)).flatten().detach().cpu().numpy(), y_test.flatten().detach().cpu().numpy()\n",
    "\n",
    "r2_test = pearsonr(pred, true)[0]**2\n",
    "\n",
    "print(f\"{model_name} model achieved test R2 = {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3165239d-c904-41a8-b3a3-f23045a02c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(os.path.join(results_path, \"R2s.csv\"), mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([[model_name, r2_test]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033f1bc-4a5e-466d-914f-e36263467fa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c504b67e-f6de-4f8b-a8b3-158343101f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from models import Transformer_torch_MHA, Transformer_2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23c819d3-ceaf-4730-9926-388e774ea692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seqs_ex = seqs + AA_size*torch.tensor(range(L))\n",
    "X = seqs_ex.to(device)\n",
    "y = phenotypes.to(device)\n",
    "X_train, y_train = X[train_list], y[train_list]\n",
    "X_val, y_val = X[val_list], y[val_list]\n",
    "X_test, y_test = X[test_list], y[test_list]\n",
    "train_dataset = ProtDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f987feff-29f8-4fd7-a46f-68220715e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Trial:\n",
    "#   Value: 0.7601\n",
    "#   Params: \n",
    "#     hidden_dim_h: 23\n",
    "#     dropout: 0.12805161023112027\n",
    "#     batch_size: 544\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccaba4c8-d314-405d-9a79-da24b19cf68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sequence_length = L\n",
    "# input_dim = AA_size*L\n",
    "# output_dim = 1\n",
    "# num_layers = 2\n",
    "# num_heads = 4\n",
    "# hidden_dim = 23*num_heads\n",
    "# dropout = 0.12805161023112027\n",
    "\n",
    "# model = Transformer_torch_MHA(L, input_dim, hidden_dim, num_layers, num_heads, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f201473-2c57-4315-8e98-25c596a640bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import pearsonr\n",
    "# learning_rate = 0.001\n",
    "# epochs = 500\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch_inputs, batch_targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_inputs)\n",
    "#         loss = criterion(outputs, batch_targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     if epoch % 20 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "#         model.eval()\n",
    "#         pred, true = model(X_test.flatten(1)).flatten().detach().cpu().numpy(), y_test.flatten().detach().cpu().numpy()\n",
    "#         print(pearsonr(pred, true)[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331ae1f-c929-427b-ae9f-1cbc0a3f1301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76019638-99cb-4e89-9a3d-555ade31f6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# learning_rate = 0.0001\n",
    "num_heads = 4\n",
    "\n",
    "sequence_length = L\n",
    "input_dim = AA_size*L\n",
    "output_dim = 1\n",
    "\n",
    "def objective(trial):\n",
    "    global criterion_best, model_best\n",
    "\n",
    "    hidden_dim_h = trial.suggest_int('hidden_dim_h', 10, 50)\n",
    "    dropout = trial.suggest_float('dropout', 0.05, 0.35)\n",
    "    batch_size = trial.suggest_int('batch_size', 100, 1200)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 30, 300)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    # learning_rate = trial.suggest_float\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    print(f\"Build model with {num_layers} layers of attention\")\n",
    "    model = Transformer_2k(L, input_dim, hidden_dim_h*num_heads, num_layers, num_heads, dropout).to(device)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_dataset,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=False)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    r2_test = []\n",
    "    try: \n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for batch_inputs, batch_targets in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_inputs)\n",
    "                    loss = criterion(outputs, batch_targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "                    model.eval()\n",
    "                    pred, true = model(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "                    print(pearsonr(pred, true)[0]**2)\n",
    "                    if pearsonr(pred, true)[0]**2 == \"nan\":\n",
    "                        break\n",
    "                    r2_test.append(pearsonr(pred, true)[0]**2)\n",
    "                    \n",
    "    except: print(\"training failed\")\n",
    "    \n",
    "    criterion = np.array(r2_test)[-1]\n",
    "    if criterion > criterion_best:\n",
    "        print(\"Found better hyperparameter, update model\")\n",
    "        criterion_best = criterion\n",
    "        model_best = model\n",
    "    \n",
    "    return np.array(r2_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e16cd-672c-45fd-a875-b55780d25c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-23 08:07:12,156] A new study created in memory with name: no-name-9ceacbef-9367-4cb3-a14b-0e722ec6a617\n",
      "/scratch/local/22019942/ipykernel_2825595/727483821.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model with 3 layers of attention\n",
      "Epoch 1/123, Loss: 0.3468048758804798\n",
      "0.5892912713597088\n",
      "Epoch 11/123, Loss: 0.2642652206122875\n",
      "0.6129965785127874\n",
      "Epoch 21/123, Loss: 0.22934292983263732\n",
      "0.6311927918564046\n",
      "Epoch 31/123, Loss: 0.21113975830376147\n",
      "0.6443502924711522\n",
      "Epoch 41/123, Loss: 0.19708516132086515\n",
      "0.6504993991633975\n",
      "Epoch 51/123, Loss: 0.18342391336336733\n",
      "0.6677516249362482\n",
      "Epoch 61/123, Loss: 0.17090581953525544\n",
      "0.6774703758202425\n",
      "Epoch 71/123, Loss: 0.15834331437945365\n",
      "0.6796323277559938\n",
      "Epoch 81/123, Loss: 0.1463141399435699\n",
      "0.6920782741135568\n",
      "Epoch 91/123, Loss: 0.13601415921002627\n",
      "0.6973881357264899\n",
      "Epoch 101/123, Loss: 0.1265990188345313\n",
      "0.702695653967123\n",
      "Epoch 111/123, Loss: 0.11875486066564918\n",
      "0.7080159577231476\n"
     ]
    }
   ],
   "source": [
    "n_trials = 100\n",
    "for num_layers in [3]:\n",
    "\n",
    "    model_name = \"TF_\" + str(num_layers)\n",
    "\n",
    "    criterion_best = 0.\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    best_trial = study.best_trial\n",
    "    print(\"Best Trial:\")\n",
    "    print(f\"  Criterion: {best_trial.value:.4f}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")  \n",
    "\n",
    "    best_hyper_parameters = {}\n",
    "    for key, value in best_trial.params.items():\n",
    "        best_hyper_parameters[key] = value\n",
    "\n",
    "    model_best.eval()\n",
    "    pred, true = model_best(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "\n",
    "    r2_test = pearsonr(pred, true)[0]**2\n",
    "    print(f\"{model_name} achieved R2 = {r2_test}\")\n",
    "\n",
    "    # save test R2 score\n",
    "    import csv\n",
    "    with open(os.path.join(results_path, \"R2s.csv\"), mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([[model_name, r2_test]])\n",
    "\n",
    "    # save predictions\n",
    "    pd.DataFrame({\"prediction\": pred, \"true\": true}).to_csv(os.path.join(results_path, model_name + \"_predictions.csv\"), index=False)\n",
    "\n",
    "    # save best model\n",
    "    torch.save(model_best, os.path.join(results_path, model_name + \"_BestModel\"))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3138b5e-3f12-4aca-9b53-a7497b15c223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_layers = 1\n",
    "# model_name = \"TF_\" + str(num_layers)\n",
    "\n",
    "# criterion_best = 0.\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# best_trial = study.best_trial\n",
    "# print(\"Best Trial:\")\n",
    "# print(f\"  Criterion: {best_trial.value:.4f}\")\n",
    "# print(\"  Params: \")\n",
    "# for key, value in best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# best_hyper_parameters = {}\n",
    "# for key, value in best_trial.params.items():\n",
    "#     best_hyper_parameters[key] = value\n",
    "\n",
    "# model_best.eval()\n",
    "# pred, true = model_best(X_val.flatten(1)).flatten().detach().cpu().numpy(), y_val.flatten().detach().cpu().numpy()\n",
    "\n",
    "# r2_test = pearsonr(pred, true)[0]**2\n",
    "# print(f\"{model_name} achieved R2 = {r2_test}\")\n",
    "\n",
    "# # save test R2 score\n",
    "# import csv\n",
    "# with open(os.path.join(results_path, \"R2s.csv\"), mode='a', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerows([[model_name, r2_test]])\n",
    "\n",
    "# # save predictions\n",
    "# pd.DataFrame({\"prediction\": pred, \"true\": true}).to_csv(os.path.join(results_path, model_name + \"_predictions.csv\"), index=False)\n",
    "\n",
    "# # save best model\n",
    "# torch.save(model_best, os.path.join(results_path, model_name + \"_BestModel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c443b-7a04-492d-a4b9-733e2d549a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
