{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacd906c-e610-4e96-bd72-592e48a5bc08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2c1977-8673-4364-9fb0-5ad9ba76515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import GPUtil\n",
    "from scipy.stats import pearsonr\n",
    "# import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "sys.path.append('../model')\n",
    "from utils import amino_acid_to_number, tokenize, Tee\n",
    "from functions import get_A2N_list, tokenize, make_train_val_test_lists_rand, prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb803aa9-b8b5-4c6d-9a89-90755e97be98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "|  1 |  0% |  0% |\n",
      "|  2 | 50% |  8% |\n",
      "|  3 | 25% |  8% |\n",
      "|  4 |  0% |  0% |\n",
      "|  5 |  0% |  0% |\n",
      "|  6 |  0% |  0% |\n",
      "|  7 |  0% | 98% |\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import GPUtil\n",
    "GPUtil.showUtilization()\n",
    "\n",
    "# torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1954dc-27a0-4cc4-8929-ca4e3410617d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# device = 'cpu'\n",
    "data_name = \"Faure2023_1_lenient\"\n",
    "in_path = \"../Data/Data_prepared/\" + data_name + \".csv\"\n",
    "datafile = pd.read_csv(in_path, index_col=None)\n",
    "phenotypes, seqs, seqs1h = prepare_data(datafile)\n",
    "_, L, AA_size = seqs1h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81dcfb6a-1082-403c-a586-5fa8d75aac5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add1(tensor):\n",
    "    return torch.cat((tensor, torch.ones(tensor.shape[0]).unsqueeze(1).to(tensor.device)), 1)\n",
    "\n",
    "def add0(tensor):\n",
    "    return torch.cat((tensor, torch.zeros(tensor.shape[0]).unsqueeze(1).to(tensor.device)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f57eee-edbc-4693-a690-0baa76c55f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.2.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KeOps] Warning : There were warnings or errors compiling formula :\n",
      "<stdin>:1:10: fatal error: cuda.h: No such file or directory\n",
      "compilation terminated.\n",
      "\n",
      "[KeOps] Warning : \n",
      "    The location of Cuda header files cuda.h and nvrtc.h could not be detected on your system.\n",
      "    You must determine their location and then define the environment variable CUDA_PATH,\n",
      "    either before launching Python or using os.environ before importing keops. For example\n",
      "    if these files are in /vol/cuda/10.2.89-cudnn7.6.4.38/include you can do :\n",
      "      import os\n",
      "      os.environ['CUDA_PATH'] = '/vol/cuda/10.2.89-cudnn7.6.4.38'\n",
      "    \n",
      "[KeOps] Compiling cuda jit compiler engine ... \n",
      "[KeOps] Warning : There were warnings or errors compiling formula :\n",
      "/home/juannanzhou/.local/lib/python3.10/site-packages/keopscore/binders/nvrtc/nvrtc_jit.cpp:5:10: fatal error: nvrtc.h: No such file or directory\n",
      "    5 | #include <nvrtc.h>\n",
      "      |          ^~~~~~~~~\n",
      "compilation terminated.\n",
      "\n",
      "OK\n",
      "[pyKeOps] Compiling nvrtc binder for python ... \n",
      "[KeOps] Warning : There were warnings or errors compiling formula :\n",
      "In file included from /home/juannanzhou/.local/lib/python3.10/site-packages/pykeops/common/keops_io/pykeops_nvrtc.cpp:4:\n",
      "/home/juannanzhou/.local/lib/python3.10/site-packages/keopscore/binders/nvrtc/keops_nvrtc.cpp:6:10: fatal error: nvrtc.h: No such file or directory\n",
      "    6 | #include <nvrtc.h>\n",
      "      |          ^~~~~~~~~\n",
      "compilation terminated.\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "from gpytorch.constraints import Positive\n",
    "from scipy.special._basic import comb\n",
    "\n",
    "\n",
    "def binom(n, k):\n",
    "    \"\"\"Compute binomial coefficient using the log-gamma function for stability.\"\"\"\n",
    "    # Ensure n and k are tensors\n",
    "    n = torch.as_tensor(n, dtype=torch.float)\n",
    "    k = torch.as_tensor(k, dtype=torch.float)\n",
    "    return torch.exp(torch.lgamma(n + 1) - torch.lgamma(k + 1) - torch.lgamma(n - k + 1))\n",
    "\n",
    "def w(k, d, alpha, l):\n",
    "    total_sum = torch.zeros_like(d, dtype=torch.float)  # Initialize the sum as a tensor of zeros with the same shape as d\n",
    "    alpha = torch.as_tensor(alpha, dtype=torch.float)  # Ensure alpha is a tensor\n",
    "    l = torch.as_tensor(l, dtype=torch.float)  # Ensure l is a tensor\n",
    "    \n",
    "    for q in range(0, k + 1):\n",
    "        q_tensor = torch.full_like(d, q, dtype=torch.float)  # Convert q to a tensor of the same shape as d\n",
    "        # Compute the term for each q where q <= d\n",
    "        term = ((-1) ** q_tensor) * ((alpha - 1) ** (k - q_tensor)) * binom(d, q_tensor) * binom(l - d, k - q_tensor)\n",
    "        # Accumulate the sum\n",
    "        total_sum += term\n",
    "    return total_sum\n",
    "\n",
    "\n",
    "lda_decay = 6\n",
    "class EpKernel(gpytorch.kernels.Kernel):\n",
    "    # the sinc kernel is stationary\n",
    "    is_stationary = True\n",
    "\n",
    "    # We will register the parameter when initializing the kernel\n",
    "    def __init__(self, n_alleles, seq_length, d_max, k_max,\n",
    "                 constrain_lda=True,\n",
    "                 log_lambdas0=None, \n",
    "                 lda_prior=None,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.alpha = n_alleles\n",
    "        self.l = seq_length\n",
    "        self.d_max = d_max\n",
    "        self.k_max = k_max\n",
    "        # register the raw parameter\n",
    "        self.register_parameter(\n",
    "            name='raw_lda', parameter=torch.nn.Parameter(lda_decay*torch.arange(1, k_max+1).float())\n",
    "        )\n",
    "        self.w_kd = self.calc_krawchouk_matrix()\n",
    "        # set the parameter constraint to be positive, when nothing is specified\n",
    "        self.constrain_lda = constrain_lda\n",
    "        \n",
    "        if constrain_lda is True:\n",
    "            lda_constraint = Positive()\n",
    "            self.register_constraint(\"raw_lda\", lda_constraint)\n",
    "\n",
    "        # register the constraint\n",
    "\n",
    "\n",
    "        # set the parameter prior, see\n",
    "        # https://docs.gpytorch.ai/en/latest/module.html#gpytorch.Module.register_prior\n",
    "        if lda_prior is not None:\n",
    "            self.register_prior(\n",
    "                \"lda_prior\",\n",
    "                lda_prior,\n",
    "                lambda m: m.lda,\n",
    "                lambda m, v : m._set_lda(v),\n",
    "            )\n",
    "    def calc_krawchouk_matrix(self):\n",
    "        d = torch.arange(self.d_max).reshape((self.d_max, 1, 1))\n",
    "        k = torch.arange(1, self.k_max + 1).reshape((1, self.k_max, 1))\n",
    "        q = torch.arange(self.k_max).reshape((1, 1, self.k_max))\n",
    "        w_kd = ((-1.)**q * (self.alpha-1.)**(k-q) * comb(d,q) * comb(self.l-d,k-q)).sum(-1) \n",
    "        return (w_kd.to(dtype=torch.float))\n",
    "    \n",
    "\n",
    "    # def get_w_d(self):\n",
    "    #     '''calculates the covariance for each hamming distance'''\n",
    "    #     lambdas = torch.exp(self.lda)\n",
    "    #     w_d = self.w_kd @ lambdas\n",
    "    #     return(w_d) # .reshape((1, 1, self.s))\n",
    "    \n",
    "    # now set up the 'actual' paramter\n",
    "    @property\n",
    "    def lda(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        if self.constrain_lda:\n",
    "            return -1*self.raw_lda_constraint.transform(self.raw_lda)\n",
    "        else: return -1*self.raw_lda\n",
    "\n",
    "    @lda.setter\n",
    "    def lda(self, value):\n",
    "        return self._set_lda(value)\n",
    "\n",
    "    def _set_lda(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_lda)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_lda=self.raw_lda_constraint.inverse_transform(value))\n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, **params):\n",
    "        # w_d = self.get_w_d()\n",
    "        w_d = self.w_kd @ torch.exp(self.lda)\n",
    "        hamming_dist = (self.covar_dist(x1, x2)**2).round()/2\n",
    "        kernel = w_d[0] * (hamming_dist == 0)\n",
    "        for d in range(1, self.d_max):\n",
    "            kernel += w_d[d] * (hamming_dist == d)\n",
    "        return kernel\n",
    "\n",
    "    \n",
    "    def get_d(self, x1, x2, **params):\n",
    "        hamming_dist = (self.covar_dist(x1, x2)**2).round()/2\n",
    "        return hamming_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea04a4f8-2a3a-4af2-880d-51b2a62afee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class alphaEpKernel(gpytorch.kernels.Kernel):\n",
    "    # the sinc kernel is stationary\n",
    "    is_stationary = True\n",
    "\n",
    "    # We will register the parameter when initializing the kernel\n",
    "    def __init__(self, n_alleles, seq_length, d_max, k_max,\n",
    "                 constrain_lda=True,\n",
    "                 log_lambdas0=None, \n",
    "                 lda_prior=None,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.alpha = n_alleles\n",
    "        self.l = seq_length\n",
    "        self.d_max = d_max\n",
    "        self.k_max = k_max\n",
    "        # register the raw parameter\n",
    "        self.register_parameter(\n",
    "            name='raw_lda', parameter=torch.nn.Parameter(lda_decay*torch.arange(1, k_max+1).float())\n",
    "        )\n",
    "        self.w_kd = self.calc_krawchouk_matrix()\n",
    "        # set the parameter constraint to be positive, when nothing is specified\n",
    "        self.constrain_lda = constrain_lda\n",
    "        \n",
    "        if constrain_lda is True:\n",
    "            lda_constraint = Positive()\n",
    "            self.register_constraint(\"raw_lda\", lda_constraint)\n",
    "\n",
    "        # register the constraint\n",
    "\n",
    "\n",
    "        # set the parameter prior, see\n",
    "        # https://docs.gpytorch.ai/en/latest/module.html#gpytorch.Module.register_prior\n",
    "        if lda_prior is not None:\n",
    "            self.register_prior(\n",
    "                \"lda_prior\",\n",
    "                lda_prior,\n",
    "                lambda m: m.lda,\n",
    "                lambda m, v : m._set_lda(v),\n",
    "            )\n",
    "    def calc_krawchouk_matrix(self):\n",
    "        d = torch.arange(self.d_max).reshape((self.d_max, 1, 1))\n",
    "        k = torch.arange(1, self.k_max + 1).reshape((1, self.k_max, 1))\n",
    "        q = torch.arange(self.k_max).reshape((1, 1, self.k_max))\n",
    "        w_kd = ((-1.)**q * (self.alpha-1.)**(k-q) * comb(d,q) * comb(self.l-d,k-q)).sum(-1) \n",
    "        return (w_kd.to(dtype=torch.float))\n",
    "    \n",
    "\n",
    "    # def get_w_d(self):\n",
    "    #     '''calculates the covariance for each hamming distance'''\n",
    "    #     lambdas = torch.exp(self.lda)\n",
    "    #     w_d = self.w_kd @ lambdas\n",
    "    #     return(w_d) # .reshape((1, 1, self.s))\n",
    "    \n",
    "    # now set up the 'actual' paramter\n",
    "    @property\n",
    "    def lda(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        if self.constrain_lda:\n",
    "            return -1*self.raw_lda_constraint.transform(self.raw_lda)\n",
    "        else: return -1*self.raw_lda\n",
    "\n",
    "    @lda.setter\n",
    "    def lda(self, value):\n",
    "        return self._set_lda(value)\n",
    "\n",
    "    def _set_lda(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_lda)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_lda=self.raw_lda_constraint.inverse_transform(value))\n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, **params):\n",
    "        x1_ = x1[:, :-1]\n",
    "        x2_ = x2[:, :-1]\n",
    "        v1 = x1[:, -1]\n",
    "        v2 = x2[:, -1]\n",
    "\n",
    "        if v2[0]==1. or v1[0]==1.:\n",
    "            print(\"Using Identity matrix as evaluate kernel\")\n",
    "            I = torch.eye(x1.shape[0], x2.shape[0]).to(x1.device)\n",
    "            return I\n",
    "        \n",
    "        else:\n",
    "            w_d = self.w_kd @ torch.exp(self.lda)\n",
    "            hamming_dist = (self.covar_dist(x1_, x2_)**2).round()/2\n",
    "            kernel = w_d[0] * (hamming_dist == 0)\n",
    "            for d in range(1, self.d_max):\n",
    "                kernel += w_d[d] * (hamming_dist == d)\n",
    "            return kernel\n",
    "\n",
    "    \n",
    "    def get_d(self, x1, x2, **params):\n",
    "        hamming_dist = (self.covar_dist(x1, x2)**2).round()/2\n",
    "        return hamming_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2030d15-ad97-4fdc-b8e7-77acd2f75112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MargKernel(gpytorch.kernels.Kernel):\n",
    "    # the sinc kernel is stationary\n",
    "    is_stationary = True\n",
    "\n",
    "    # We will register the parameter when initializing the kernel\n",
    "    def __init__(self, n_alleles, seq_length,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = n_alleles\n",
    "        self.l = seq_length\n",
    "        self.d_max = d_max\n",
    "        self.k_max = k_max\n",
    "        self.w_d = torch.tensor([AA_size - 1, -1])\n",
    "\n",
    "    def forward(self, x1, x2, site, **params):\n",
    "        hamming_dist = (self.covar_dist(x1[:, site*AA_size:((site + 1)*AA_size)], \n",
    "                                        x2[:, site*AA_size:((site + 1)*AA_size)])**2).round()/2\n",
    "        \n",
    "        kernel = -1*torch.ones(*hamming_dist.shape).to(hamming_dist.device)\n",
    "        kernel += AA_size * (hamming_dist == 0)\n",
    "        \n",
    "        return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8824b9f9-b384-463c-942a-19e00a6318b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_max = L\n",
    "k_max = 8\n",
    "# Use the simplest form of GP model, exact inference\n",
    "lda_prior_cov = (1 / torch.exp(torch.arange(k_max))*torch.eye(k_max)).to(device)\n",
    "class EpModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, get_alpha=False):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        if get_alpha: \n",
    "            self.covar_module = alphaEpKernel(AA_size, L, d_max, k_max)\n",
    "        else: \n",
    "            self.covar_module = EpKernel(AA_size, L, d_max, k_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86991ac-60cc-42f8-9532-af7bea2d3f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import kernels\n",
    "# importlib.reload(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a914bf8-08e4-465e-bc79-55c23b4ac846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ProtDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m seqs_ex\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m phenotypes\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mProtDataset\u001b[49m(X, y)\n\u001b[1;32m      9\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(eval_dataset,\n\u001b[1;32m     10\u001b[0m                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                                drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ProtDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# data for Transformer model prediction\n",
    "\n",
    "seqs_ex = seqs + AA_size*torch.tensor(range(L))\n",
    "X = seqs_ex.to(device)\n",
    "y = phenotypes.to(device)\n",
    "\n",
    "\n",
    "eval_dataset = ProtDataset(X, y)\n",
    "eval_loader = data.DataLoader(eval_dataset,\n",
    "                               batch_size=1000,\n",
    "                               shuffle=False,\n",
    "                               drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ee647-8ea1-4f3f-a0bd-4371e16484d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80e5bd-2027-43cb-92af-3aace9a8d250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_max = L\n",
    "k_max = 8\n",
    "# Use the simplest form of GP model, exact inference\n",
    "lda_prior_cov = (1 / torch.exp(torch.arange(k_max))*torch.eye(k_max)).to(device)\n",
    "class EpModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, get_alpha=False):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        if get_alpha: \n",
    "            self.covar_module = alphaEpKernel(AA_size, L, d_max, k_max)\n",
    "        else: \n",
    "            self.covar_module = EpKernel(AA_size, L, d_max, k_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c5c75a6-85a0-47c2-8345-77ded70c4b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sub_list = random.sample(range(len(datafile)), 20000)\n",
    "train_list = random.sample(sub_list, int(.99*len(sub_list)))\n",
    "\n",
    "val_list = set(sub_list).difference(set(train_list))\n",
    "val_list = list(val_list)\n",
    "\n",
    "train_x = seqs1h[train_list].float().flatten(1)\n",
    "train_y = torch.ones(train_x.shape[0])\n",
    "\n",
    "train_x = train_x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12435939-1809-48ca-82d1-1a670945c39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_lda_opt = torch.nn.Parameter(torch.tensor([ 3.7921,  7.7892, 11.7760, 15.7680, 19.7600, 23.7520, 27.7440, 32.1356]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c2834c-08dd-40dd-953f-551e996bfafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "\n",
    "model = EpModel(train_x, train_y, likelihood, get_alpha=False).cuda()\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "model.covar_module.w_kd = model.covar_module.w_kd.to(device)\n",
    "model.covar_module.raw_lda = raw_lda_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d3fbaf6-a021-45f5-96b3-f1b61aef4d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mvn = model(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cd95467-ce17-48ac-ad7a-ac0a8387ebb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = mvn.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae37a92-64ef-4ecc-9be0-082da8c9bbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19800])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94708343-d9b5-4002-8ecf-cf40fecf4062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = y.unsqueeze(1).matmul(y.unsqueeze(1).T)\n",
    "Y = Y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b634f20b-e41f-472d-8eb8-d347884a8b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = model.covar_module(train_x).evaluate().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcade5f-dab8-43c1-a575-2c28c5d91007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x14e02cbf1de0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(Y.cpu().detach().numpy(), K.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539dbd6a-1e7e-4a1a-8922-d0d8c36f8532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.15744163861751637, pvalue=0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(Y.cpu().detach().numpy(), K.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29473c2-52da-470c-99f6-f41ff0dd2cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
